{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "from sentence_transformers.losses import CosineSimilarityLoss\n",
    "from setfit import SetFitModel, SetFitTrainer\n",
    "\n",
    "from src.misc import paths\n",
    "\n",
    "\n",
    "source_model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "target_model_name = f\"{source_model_name}-denoise\"\n",
    "\n",
    "\n",
    "def model_init(params: Dict = None) -> SetFitModel:\n",
    "    params = params or {}\n",
    "    params = {\n",
    "        \"head_params\": {\n",
    "            \"max_iter\": params.get(\"max_iter\", 256),\n",
    "            \"solver\":   params.get(\"solver\", \"liblinear\"),\n",
    "        }\n",
    "    }\n",
    "    model = SetFitModel.from_pretrained(source_model_name, **params)\n",
    "    return model\n",
    "\n",
    "\n",
    "def hp_space(trial) -> Dict:\n",
    "    return {\n",
    "        \"learning_rate\":  trial.suggest_float(\"learning_rate\", 1e-6, 1e-4, log=True),\n",
    "        \"num_epochs\":     trial.suggest_categorical(\"num_epochs\", [1]),\n",
    "        \"batch_size\":     trial.suggest_categorical(\"batch_size\", [16, 32]),\n",
    "        \"num_iterations\": trial.suggest_categorical(\"num_iterations\", [16, 32]),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tony_not_noise</th>\n",
       "      <th>anne_not_noise</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>label_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>So the overall investment amounted to some RUB...</td>\n",
       "      <td>0</td>\n",
       "      <td>keep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>We remain well positioned at the end of second...</td>\n",
       "      <td>0</td>\n",
       "      <td>keep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I hope this slide makes it clear that Coeur's ...</td>\n",
       "      <td>0</td>\n",
       "      <td>keep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>And then, obviously, on the Outotec businesses...</td>\n",
       "      <td>0</td>\n",
       "      <td>keep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The budget also includes $18.3 million for San...</td>\n",
       "      <td>0</td>\n",
       "      <td>keep</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tony_not_noise  anne_not_noise  \\\n",
       "0             NaN             NaN   \n",
       "1             NaN             NaN   \n",
       "2             NaN             NaN   \n",
       "3             NaN             NaN   \n",
       "4             NaN             NaN   \n",
       "\n",
       "                                                text  label label_name  \n",
       "0  So the overall investment amounted to some RUB...      0       keep  \n",
       "1  We remain well positioned at the end of second...      0       keep  \n",
       "2  I hope this slide makes it clear that Coeur's ...      0       keep  \n",
       "3  And then, obviously, on the Outotec businesses...      0       keep  \n",
       "4  The budget also includes $18.3 million for San...      0       keep  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_pd = pd.read_csv(paths.data / \"denoise\" / \"denoise-validated.csv\")\n",
    "dataset_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ds = Dataset.from_pandas(dataset_pd) \\\n",
    "                    .train_test_split(test_size=0.2, seed=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b8e4eb4e4d5444096b13f2e4a4dcaeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n",
      "\u001b[32m[I 2023-04-04 01:59:21,558]\u001b[0m A new study created in memory with name: no-name-379d06b4-18b0-45e4-a29d-3aae79f2e8ef\u001b[0m\n",
      "Trial: {'learning_rate': 8.500393185376232e-06, 'num_epochs': 1, 'batch_size': 32, 'num_iterations': 32}\n",
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 13056\n",
      "  Num epochs = 1\n",
      "  Total optimization steps = 408\n",
      "  Total train batch size = 32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2bbcf6f83c743eba56a4bd80d318132",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8657eccdfd4e4b798d2452681557b686",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running evaluation *****\n",
      "\u001b[32m[I 2023-04-04 02:07:17,835]\u001b[0m Trial 0 finished with value: 0.7647058823529411 and parameters: {'learning_rate': 8.500393185376232e-06, 'num_epochs': 1, 'batch_size': 32, 'num_iterations': 32}. Best is trial 0 with value: 0.7647058823529411.\u001b[0m\n",
      "Trial: {'learning_rate': 1.8736779609851885e-06, 'num_epochs': 1, 'batch_size': 16, 'num_iterations': 32}\n",
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 13056\n",
      "  Num epochs = 1\n",
      "  Total optimization steps = 816\n",
      "  Total train batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39c9a8a51ee0444aaa3c983c3545ca0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b57a2064059a4347bc64cf60f473ede9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/816 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running evaluation *****\n",
      "\u001b[32m[I 2023-04-04 02:15:43,906]\u001b[0m Trial 1 finished with value: 0.6875000000000001 and parameters: {'learning_rate': 1.8736779609851885e-06, 'num_epochs': 1, 'batch_size': 16, 'num_iterations': 32}. Best is trial 0 with value: 0.7647058823529411.\u001b[0m\n",
      "Trial: {'learning_rate': 1.077715639077575e-05, 'num_epochs': 1, 'batch_size': 16, 'num_iterations': 16}\n",
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 6528\n",
      "  Num epochs = 1\n",
      "  Total optimization steps = 408\n",
      "  Total train batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "972640bb0fbe484b9696a93dfd34d3f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8ea3eede2fc483e959d21d77f245252",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running evaluation *****\n",
      "\u001b[32m[I 2023-04-04 02:19:54,542]\u001b[0m Trial 2 finished with value: 0.7272727272727272 and parameters: {'learning_rate': 1.077715639077575e-05, 'num_epochs': 1, 'batch_size': 16, 'num_iterations': 16}. Best is trial 0 with value: 0.7647058823529411.\u001b[0m\n",
      "Trial: {'learning_rate': 3.338073147619401e-05, 'num_epochs': 1, 'batch_size': 32, 'num_iterations': 32}\n",
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 13056\n",
      "  Num epochs = 1\n",
      "  Total optimization steps = 408\n",
      "  Total train batch size = 32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7f66e846c7c48409178b1995dfc138e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f189d56e0344ab1b0645f2df8f4e739",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running evaluation *****\n",
      "\u001b[32m[I 2023-04-04 02:27:35,462]\u001b[0m Trial 3 finished with value: 0.7272727272727272 and parameters: {'learning_rate': 3.338073147619401e-05, 'num_epochs': 1, 'batch_size': 32, 'num_iterations': 32}. Best is trial 0 with value: 0.7647058823529411.\u001b[0m\n",
      "Trial: {'learning_rate': 1.664855845283107e-05, 'num_epochs': 1, 'batch_size': 16, 'num_iterations': 16}\n",
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 6528\n",
      "  Num epochs = 1\n",
      "  Total optimization steps = 408\n",
      "  Total train batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "555cef696d59451282dab4e8e5c2465e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "608e5603181a4b829ca97a53979cd45f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running evaluation *****\n",
      "\u001b[32m[I 2023-04-04 02:31:44,590]\u001b[0m Trial 4 finished with value: 0.7272727272727272 and parameters: {'learning_rate': 1.664855845283107e-05, 'num_epochs': 1, 'batch_size': 16, 'num_iterations': 16}. Best is trial 0 with value: 0.7647058823529411.\u001b[0m\n",
      "Trial: {'learning_rate': 1.0984717119181637e-05, 'num_epochs': 1, 'batch_size': 16, 'num_iterations': 16}\n",
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 6528\n",
      "  Num epochs = 1\n",
      "  Total optimization steps = 408\n",
      "  Total train batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae07fd518dca483dbd7915e59482be61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11d97c0b21f34bfd9c3e62126020875b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running evaluation *****\n",
      "\u001b[32m[I 2023-04-04 02:34:46,509]\u001b[0m Trial 5 finished with value: 0.7272727272727272 and parameters: {'learning_rate': 1.0984717119181637e-05, 'num_epochs': 1, 'batch_size': 16, 'num_iterations': 16}. Best is trial 0 with value: 0.7647058823529411.\u001b[0m\n",
      "Trial: {'learning_rate': 1.0658404027496414e-05, 'num_epochs': 1, 'batch_size': 16, 'num_iterations': 16}\n",
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 6528\n",
      "  Num epochs = 1\n",
      "  Total optimization steps = 408\n",
      "  Total train batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d510c48f679941b1b4ea689510683d73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f401a234454b474e9c0d4f400fc6726a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running evaluation *****\n",
      "\u001b[32m[I 2023-04-04 02:37:40,302]\u001b[0m Trial 6 finished with value: 0.7272727272727272 and parameters: {'learning_rate': 1.0658404027496414e-05, 'num_epochs': 1, 'batch_size': 16, 'num_iterations': 16}. Best is trial 0 with value: 0.7647058823529411.\u001b[0m\n",
      "Trial: {'learning_rate': 9.783528150044016e-05, 'num_epochs': 1, 'batch_size': 32, 'num_iterations': 32}\n",
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 13056\n",
      "  Num epochs = 1\n",
      "  Total optimization steps = 408\n",
      "  Total train batch size = 32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be583ed0c37143dbbcdf44c9b8b261eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "645b23302a24480582d0c36606bc7b17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running evaluation *****\n",
      "\u001b[32m[I 2023-04-04 02:43:05,450]\u001b[0m Trial 7 finished with value: 0.7878787878787877 and parameters: {'learning_rate': 9.783528150044016e-05, 'num_epochs': 1, 'batch_size': 32, 'num_iterations': 32}. Best is trial 7 with value: 0.7878787878787877.\u001b[0m\n",
      "Trial: {'learning_rate': 1.543319881556778e-05, 'num_epochs': 1, 'batch_size': 32, 'num_iterations': 16}\n",
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 6528\n",
      "  Num epochs = 1\n",
      "  Total optimization steps = 204\n",
      "  Total train batch size = 32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4afc7906b7d40f9b0ffe22141add80c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c53fb89e441e4783808a911c7b979cca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/204 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running evaluation *****\n",
      "\u001b[32m[I 2023-04-04 02:45:50,073]\u001b[0m Trial 8 finished with value: 0.7272727272727272 and parameters: {'learning_rate': 1.543319881556778e-05, 'num_epochs': 1, 'batch_size': 32, 'num_iterations': 16}. Best is trial 7 with value: 0.7878787878787877.\u001b[0m\n",
      "Trial: {'learning_rate': 2.8639426399654712e-05, 'num_epochs': 1, 'batch_size': 32, 'num_iterations': 32}\n",
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 13056\n",
      "  Num epochs = 1\n",
      "  Total optimization steps = 408\n",
      "  Total train batch size = 32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "177efd542f434b8ba1c5f9c4bc3c9a49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4605168e36b451985db00715c23295f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running evaluation *****\n",
      "\u001b[32m[I 2023-04-04 02:51:15,251]\u001b[0m Trial 9 finished with value: 0.7647058823529411 and parameters: {'learning_rate': 2.8639426399654712e-05, 'num_epochs': 1, 'batch_size': 32, 'num_iterations': 32}. Best is trial 7 with value: 0.7878787878787877.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "trainer = SetFitTrainer(train_dataset=dataset_ds[\"train\"],\n",
    "                        eval_dataset=dataset_ds[\"test\"],\n",
    "                        model_init=model_init,\n",
    "                        loss_class=CosineSimilarityLoss,\n",
    "                        metric=\"f1\",\n",
    "                        seed=42)\n",
    "best_run = trainer.hyperparameter_search(hp_space, n_trials=10, direction=\"maximize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 13056\n",
      "  Num epochs = 1\n",
      "  Total optimization steps = 408\n",
      "  Total train batch size = 32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5806ea2b015c4b8eb419b688cf15687c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "543047c063214d238d0c5a7c89ecf2a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.apply_hyperparameters(best_run.hyperparameters, final_model=True)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained(os.path.join(paths.model / target_model_name))\n",
    "model = SetFitModel.from_pretrained(os.path.join(paths.model / target_model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noise: Thank you, Andrew, and good afternoon to everyone.\n",
      "noise: Following the remarks made by management, we'll open the call for Q&A.\n",
      "noise: Welcome to our third quarter 2015 earnings call.\n",
      "noise: It is available during this call on our website under the SEC Filings tab.\n",
      "noise: We will now begin the question-and-answer session.\n",
      "keep: With us today are our Chairman and CEO, Greg Boyce; Executive Vice President and Chief Financial Officer, Mike Crews; and President and Chief Commercial Officer, Rick Navarre.We do have some forward-looking statements, and they should be considered along with the risk factors that we note at the end of our release, as well as the MD&A sections of our filed documents.\n",
      "noise: And that's what's really happening.\n",
      "keep: And as a Phil mentioned, I started my career with Hecla Mining Company over 40 years ago, and I'm pleased to be ending it here with the company reporting record financial results.Slide 6 sets forth a few key production and financial metrics.\n",
      "keep: So it is an efficient use of capital.Moving on to San Sebastian.\n",
      "noise: John, let me interrupt you.\n",
      "noise: Look forward to hearing from you.\n",
      "noise: Okay, well, I think that -- I'm sorry, go ahead operator.\n",
      "noise: We're available if you have any further questions, call Mike or call me.\n",
      "noise: Thanks very much for joining us today.\n",
      "keep: And they should be considered along with the risk factors that we note at the end of our release, as well as the MD&A sections of our filed documents.\n",
      "noise: Following the remarks made by management, we'll open the call for Q&A.\n",
      "noise: Welcome to our third quarter 2015 earnings call.\n",
      "noise: It is available during this call on our website under the SEC Filings tab.\n",
      "noise: We will now begin the question-and-answer session.\n",
      "noise: And that has been a great thing for us to see.\n",
      "noise: I hope you and your families are doing well, both physically and mentally.\n",
      "keep: So while there's a lot of discussion about price sensitivity, they need the coal.\n",
      "keep: With the silver price up about $3 from the first quarter to the second, second quarter is on a path of significant growth in our cash flow, but I'm getting ahead of myself.To have these results, we've maintained investment in all of our properties.\n",
      "keep: At this point, the focus is on extending what we're doing now, which is using the leased mill and contract miner.\n",
      "keep: We've got the excavation done.\n",
      "keep: We still have another 6, 8 months of work that we have to do to equip it.\n",
      "keep: We're now over 7 months into this COVID-19 pandemic, and I'm happy to say the company is in good shape, and we're excited about the future of our industry.\n",
      "keep: In fact, I would say that over the course of this year, our belief in a bright future for our industry has strengthened.\n",
      "keep: That's why we remain a pure-play supplier of the uranium fuel needed to produce clean, carbon-free baseload electricity.\n",
      "keep: We also remain very bullish on the uranium market.\n",
      "keep: Well, first, around the globe, we're seeing an increasing focus on electrification for various reasons.\n",
      "keep: There are those that are installing baseload power.\n",
      "keep: Then there are those who are looking for a reliable replacement to fossil fuel sources.\n",
      "keep: And finally, there's new demand for things like the electrification of transportation.\n",
      "keep: This is occurring precisely at the same time countries around the world are focused on decarbonization.\n",
      "keep: And that has led to the recognition from a policy point of view that nuclear will be needed in the toolbox to sustainably achieve both electrification and decarbonization at the same time.\n",
      "keep: China, for example, who has a goal to have 25 million electric vehicles on the road by 2030, recently stated that its objective is to become carbon neutral before 2060.\n",
      "keep: The follow-on study from a climate scientist in that country predicted that to achieve this goal will require an estimated quadrupling of nuclear power capacity in that country.\n",
      "keep: That would be about 200 reactors for China alone, double that of the U.S. fleet, which is currently the largest in the world.\n",
      "keep: So demand for nuclear is increasing.\n"
     ]
    }
   ],
   "source": [
    "texts = [\"Thank you, Andrew, and good afternoon to everyone.\",\n",
    "         \"Following the remarks made by management, we'll open the call for Q&A.\",\n",
    "         \"Welcome to our third quarter 2015 earnings call.\",\n",
    "         \"It is available during this call on our website under the SEC Filings tab.\",\n",
    "         \"We will now begin the question-and-answer session.\",\n",
    "         \"With us today are our Chairman and CEO, Greg Boyce; Executive Vice President and Chief Financial Officer, Mike Crews; and President and Chief Commercial Officer, Rick Navarre.We do have some forward-looking statements, and they should be considered along with the risk factors that we note at the end of our release, as well as the MD&A sections of our filed documents.\",\n",
    "         \"And that's what's really happening.\",\n",
    "         \"And as a Phil mentioned, I started my career with Hecla Mining Company over 40 years ago, and I'm pleased to be ending it here with the company reporting record financial results.Slide 6 sets forth a few key production and financial metrics.\",\n",
    "         \"So it is an efficient use of capital.Moving on to San Sebastian.\",\n",
    "         \"John, let me interrupt you.\",\n",
    "         \"Look forward to hearing from you.\",\n",
    "         \"Okay, well, I think that -- I'm sorry, go ahead operator.\",\n",
    "         \"We're available if you have any further questions, call Mike or call me.\",\n",
    "         \"Thanks very much for joining us today.\",    \n",
    "         \"And they should be considered along with the risk factors that we note at the end of our release, as well as the MD&A sections of our filed documents.\",    \"Following the remarks made by management, we'll open the call for Q&A.\",    \"Welcome to our third quarter 2015 earnings call.\",    \"It is available during this call on our website under the SEC Filings tab.\",    \n",
    "         \"We will now begin the question-and-answer session.\",\n",
    "         \"And that has been a great thing for us to see.\",\n",
    "         'I hope you and your families are doing well, both physically and mentally.',\n",
    "         \"So while there's a lot of discussion about price sensitivity, they need the coal.\",\n",
    "         \"With the silver price up about $3 from the first quarter to the second, second quarter is on a path of significant growth in our cash flow, but I'm getting ahead of myself.To have these results, we've maintained investment in all of our properties.\",\n",
    "         \"At this point, the focus is on extending what we're doing now, which is using the leased mill and contract miner.\",\n",
    "         \"We've got the excavation done.\",\n",
    "         \"We still have another 6, 8 months of work that we have to do to equip it.\", \n",
    "         \"We're now over 7 months into this COVID-19 pandemic, and I'm happy to say the company is in good shape, and we're excited about the future of our industry.\", \n",
    "         'In fact, I would say that over the course of this year, our belief in a bright future for our industry has strengthened.', \n",
    "         \"That's why we remain a pure-play supplier of the uranium fuel needed to produce clean, carbon-free baseload electricity.\", \n",
    "         'We also remain very bullish on the uranium market.', \n",
    "         \"Well, first, around the globe, we're seeing an increasing focus on electrification for various reasons.\", \n",
    "         'There are those that are installing baseload power.', \n",
    "         'Then there are those who are looking for a reliable replacement to fossil fuel sources.', \n",
    "         \"And finally, there's new demand for things like the electrification of transportation.\", \n",
    "         'This is occurring precisely at the same time countries around the world are focused on decarbonization.', \n",
    "         'And that has led to the recognition from a policy point of view that nuclear will be needed in the toolbox to sustainably achieve both electrification and decarbonization at the same time.', \n",
    "         'China, for example, who has a goal to have 25 million electric vehicles on the road by 2030, recently stated that its objective is to become carbon neutral before 2060.', \n",
    "         'The follow-on study from a climate scientist in that country predicted that to achieve this goal will require an estimated quadrupling of nuclear power capacity in that country.', \n",
    "         'That would be about 200 reactors for China alone, double that of the U.S. fleet, which is currently the largest in the world.', \n",
    "         'So demand for nuclear is increasing.']\n",
    "\n",
    "for p, s in zip(model(texts), texts):\n",
    "    print(f\"{'noise' if p else 'keep'}: {s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wharton",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
